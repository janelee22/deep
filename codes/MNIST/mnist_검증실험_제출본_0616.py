# -*- coding: utf-8 -*-
"""MNIST_검증실험_제출본_0616

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qfS32D2ocDZg3219X8sDX_OF5OcFFhUR

#import
"""

import copy
import itertools
import numpy as np
import tensorflow as tf
from tensorflow.nn import softmax
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, SimpleRNNCell,Flatten
from tensorflow.keras import Input
from tensorflow.keras.layers import RNN, SimpleRNNCell, LSTMCell, GRUCell

from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np
from sklearn.cluster import KMeans

from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam

from tensorflow.keras.layers import Dense, Flatten, SimpleRNNCell, Conv2D

import random

from tensorflow.keras.layers import SimpleRNN, Dense, Input



"""#DRNN"""

def dRNN(cell, inputs, rate, scope='default'):
    n_steps = len(inputs)
    if rate < 0 or rate >= n_steps:
        raise ValueError('The \'rate\' variable needs to be adjusted.')
    print("Building layer: %s, input length: %d, dilation rate: %d, input dim: %d." % (scope, n_steps, rate, inputs[0].shape[1]))

    # make the length of inputs divide 'rate', by using zero-padding
    EVEN = (n_steps % rate) == 0
    if not EVEN:
        zero_tensor = tf.zeros_like(inputs[0])
        dialated_n_steps = n_steps // rate + 1
        print("=====> %d time points need to be padded. " % (dialated_n_steps * rate - n_steps))
        print("=====> Input length for sub-RNN: %d" % (dialated_n_steps))
        for i_pad in range(dialated_n_steps * rate - n_steps):
            inputs.append(zero_tensor)
    else:
        dialated_n_steps = n_steps // rate
        print("=====> Input length for sub-RNN: %d" % (dialated_n_steps))

    dilated_inputs = [tf.concat(inputs[i * rate:(i + 1) * rate], axis=0) for i in range(dialated_n_steps)]
    dilated_outputs = tf.keras.layers.RNN(cell, return_sequences=True, return_state=False, go_backwards=False,
                                          stateful=False, time_major=True, unroll=False,
                                          input_shape=(dialated_n_steps, inputs[0].shape[1]),
                                          name=scope)(tf.stack(dilated_inputs))
    splitted_outputs = tf.split(dilated_outputs, num_or_size_splits=rate, axis=1)
    unrolled_outputs = tf.unstack(tf.concat(splitted_outputs, axis=0))
    outputs = unrolled_outputs[:n_steps]

    return outputs



"""#Mymodel"""

class MyModel(Model):
    def __init__(self, n_classes):
        super(MyModel, self).__init__()

        self.softmax = tf.keras.activations.softmax
        self.cells = [LSTMCell(20) for _ in range(4)]  # 수정: 4개의 하위 네트워크

        self.classifier = Dense(n_classes, activation='softmax')
        self.dense1 = Dense(128, activation='relu')
        self.flatten = Flatten()
        self.conv2 = Conv2D(filters=10, kernel_size=(1, 4), strides=(1, 1), padding='valid',
                            input_shape=(1, None, 1))

    def call(self, x):
        outputs = []
        for i in range(4):  # 수정: 4개의 하위 네트워크
            x_sub = dRNN(self.cells[i], list(x), 2 ** i)
            x_sub = tf.stack(x_sub, axis=-1)
            x_sub = tf.expand_dims(x_sub, axis=1)

            x_sub = self.conv2(x_sub)
            x_sub = self.flatten(x_sub)

            outputs.append(x_sub)

        x = tf.concat(outputs, axis=-1)
        x = tf.reduce_sum(x, axis=-1)  # Sum the outputs of the sub-networks
        x = tf.expand_dims(x, axis=1)

        return self.classifier(x)

"""#Dataset"""

#Unpermute
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((train_images.shape[0], 28 * 28)) / 255.0
test_images = test_images.reshape((test_images.shape[0], 28 * 28)) / 255.0

#Permuted
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_labels_categorical = to_categorical(train_labels)
test_labels_categorical = to_categorical(test_labels)
# Reshape the data to 2D for SimpleRNN
train_images = train_images.reshape((train_images.shape[0], 28*28,1 ))
test_images = test_images.reshape((test_images.shape[0], 28*28, 1))

# Scale the data
train_images = train_images / 255.0
test_images = test_images / 255.0

np.random.seed(0)  # Use a fixed seed for reproducibility
permutation = np.random.permutation(train_images.shape[1])

# Apply the permutation to the data
x_train_permuted = train_images[:, permutation,:]
x_test_permuted = test_images[:, permutation,:]

train_images=x_train_permuted
train_images = train_images.reshape((train_images.shape[0], 28 * 28)) / 255.0
test_images = test_images.reshape((test_images.shape[0], 28 * 28)) / 255.0

###noisy
##noisy

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Reshape the MNIST dataset from (num_samples, 28, 28) to (num_samples, 784)
x_train = x_train.reshape((x_train.shape[0], -1))
x_test = x_test.reshape((x_test.shape[0], -1))

# Normalize the pixel values to be between 0 and 1
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Pad the sequences to the length of 1000 with zeros
x_train_padded = np.zeros((x_train.shape[0], 1000))
x_test_padded = np.zeros((x_test.shape[0], 1000))

x_train_padded[:, :x_train.shape[1]] = x_train
x_test_padded[:, :x_test.shape[1]] = x_test

# Generate the noise
noise_train = np.random.uniform(0, 1, (x_train.shape[0], 1000 - x_train.shape[1]))
noise_test = np.random.uniform(0, 1, (x_test.shape[0], 1000 - x_test.shape[1]))

# Add the noise to the padded MNIST data
x_train_padded[:, x_train.shape[1]:] = noise_train
x_test_padded[:, x_test.shape[1]:] = noise_test
train_labels_categorical = to_categorical(y_train, num_classes=10)
test_labels_categorical = to_categorical(y_test, num_classes=10)

"""#Cond1"""

#유사한 이미지 인접배치
# k-means clustering
kmeans = KMeans(n_clusters=5, random_state=2023)
kmeans.fit(train_images)
cluster_labels = kmeans.labels_

clusters = {}
for i, label in enumerate(cluster_labels):
    if label not in clusters:
        clusters[label] = []
    clusters[label].append(i)

restructured_train_images = []
restructured_train_labels = []

max_length = max(len(clusters[i]) for i in range(5))

for i in range(max_length):
    for cluster_name in range(5):  # 0, 1, 2, 3, 4
        cluster = clusters[cluster_name]
        if i < len(cluster):
            unit = cluster[i]
            restructured_train_images.append(train_images[unit])
            restructured_train_labels.append(train_labels[unit])

restructured_train_images = np.array(restructured_train_images)
restructured_train_labels = np.array(restructured_train_labels)

# Convert to tensors
restructured_train_images = tf.convert_to_tensor(restructured_train_images, dtype=tf.float32)
restructured_train_labels = tf.convert_to_tensor(restructured_train_labels, dtype=tf.uint8)
train_labels_categorical = to_categorical(restructured_train_labels, num_classes=10)
test_labels_categorical = to_categorical(test_labels, num_classes=10)
restructured_train_images = tf.reshape(restructured_train_images, (60000, 784, 1))
test_images = tf.reshape(test_images, (-1, 784, 1))

# Function for reformatting the data for RNN
def _rnn_reformat(x, input_dims, n_steps):
    x_ = tf.transpose(x, [1, 0, 2])
    x_ = tf.reshape(x_, [-1, input_dims])
    x_reformat = tf.split(x_, n_steps, 0)
    return x_reformat

# Reformat the data for RNN
x_reformat = _rnn_reformat(restructured_train_images, 1, 28 * 28)
x_reformat_test = _rnn_reformat(test_images, 1, 28 * 28)

# Create and compile the model
model = MyModel(10)
model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(x_reformat, train_labels_categorical, batch_size=128, epochs=10, validation_data=(x_reformat_test, test_labels_categorical))

model.summary()

print(model.evaluate(x_reformat_test, test_labels_categorical))



"""#Cond2"""

# 데이터 로드
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((train_images.shape[0], 28 * 28)) / 255.0
test_images = test_images.reshape((test_images.shape[0], 28 * 28)) / 255.0

# k-means 클러스터링
kmeans = KMeans(n_clusters=5, random_state=2023)
kmeans.fit(train_images)
cluster_labels = kmeans.labels_

clusters = {}
for i, label in enumerate(cluster_labels):
    if label not in clusters:
        clusters[label] = []
    clusters[label].append(i)

restructured_train_images = []
restructured_train_labels = []

# 각 클러스터에서 동일한 개수의 데이터를 선택하여 재구성
max_length = max(len(clusters[i]) for i in range(5))

for i in range(max_length):
    for cluster_name in range(5):  # 0, 1, 2, 3, 4
        cluster = clusters[cluster_name]
        if i < len(cluster):
            unit = cluster[i]
            restructured_train_images.append(train_images[unit])
            restructured_train_labels.append(train_labels[unit])

restructured_train_images = np.array(restructured_train_images)
restructured_train_labels = np.array(restructured_train_labels)

# 텐서로 변환
restructured_train_images = tf.convert_to_tensor(restructured_train_images, dtype=tf.float32)
restructured_train_labels = tf.convert_to_tensor(restructured_train_labels, dtype=tf.uint8)
train_labels_categorical = to_categorical(restructured_train_labels, num_classes=10)
test_labels_categorical = to_categorical(test_labels, num_classes=10)

restructured_train_images = tf.reshape(restructured_train_images, (60000, 784, 1))
test_images = tf.reshape(test_images, (-1, 784, 1))

def _rnn_reformat(x, input_dims, n_steps):


    x_ = tf.transpose(x, [1, 0, 2])

    x_ = tf.reshape(x_, [-1, input_dims])

    x_reformat = tf.split(x_, n_steps, 0)

    return x_reformat

x_reformat=_rnn_reformat(restructured_train_images, 1, 28*28)
x_reformat_test=_rnn_reformat(test_images , 1, 28*28)

# Create and compile the model
model = MyModel(10)
model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(x_reformat, train_labels_categorical, batch_size=128, epochs=10, validation_data=(x_reformat_test, test_labels_categorical))

model.summary()

print(model.evaluate(x_reformat_test, test_labels_categorical))



"""# Cond3"""

# k-means 클러스터링
kmeans = KMeans(n_clusters=5, random_state=2023)
kmeans.fit(train_images)
cluster_labels = kmeans.labels_

clusters = {}
for i, label in enumerate(cluster_labels):
    if label not in clusters:
        clusters[label] = []
    clusters[label].append(i)

# First, combine all the data from the groups
combined_data = []
for cluster_name in range(5):  # 0, 1, 2, 3, 4
    cluster = clusters[cluster_name]
    for unit in cluster:
        combined_data.append((train_images[unit], train_labels[unit]))

# Now, shuffle the combined data
random.shuffle(combined_data)

# Finally, split the shuffled data back into images and labels
restructured_train_images, restructured_train_labels = zip(*combined_data)

restructured_train_images = np.array(restructured_train_images)
restructured_train_labels = np.array(restructured_train_labels)

# 텐서로 변환
restructured_train_images = tf.convert_to_tensor(restructured_train_images, dtype=tf.float32)
restructured_train_labels = tf.convert_to_tensor(restructured_train_labels, dtype=tf.uint8)
train_labels_categorical = to_categorical(restructured_train_labels, num_classes=10)
test_labels_categorical = to_categorical(test_labels, num_classes=10)

# 데이터 로드
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((train_images.shape[0], 28 * 28)) / 255.0
test_images = test_images.reshape((test_images.shape[0], 28 * 28)) / 255.0

# 데이터 섞기
combined_data = list(zip(train_images, train_labels))
random.shuffle(combined_data)

restructured_train_images, restructured_train_labels = zip(*combined_data)

restructured_train_images = np.array(restructured_train_images)
restructured_train_labels = np.array(restructured_train_labels)

# 텐서로 변환
restructured_train_images = tf.convert_to_tensor(restructured_train_images, dtype=tf.float32)
restructured_train_labels = tf.convert_to_tensor(restructured_train_labels, dtype=tf.uint8)
train_labels_categorical = to_categorical(restructured_train_labels, num_classes=10)
test_labels_categorical = to_categorical(test_labels, num_classes=10)

restructured_train_images = tf.reshape(restructured_train_images, (60000, 784, 1))
test_images = tf.reshape(test_images, (-1, 784, 1))

def _rnn_reformat(x, input_dims, n_steps):


    x_ = tf.transpose(x, [1, 0, 2])

    x_ = tf.reshape(x_, [-1, input_dims])

    x_reformat = tf.split(x_, n_steps, 0)

    return x_reformat

x_reformat=_rnn_reformat(restructured_train_images, 1, 28*28)
x_reformat_test=_rnn_reformat(test_images , 1, 28*28)
model = MyModel(10)
model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x_reformat,train_labels_categorical, batch_size=128, epochs=10, validation_data=(x_reformat_test, test_labels_categorical))

model.summary()

print(model.evaluate(x_reformat_test, test_labels_categorical))

