VCTK_rep_MFCC.py
# -*- coding: utf-8 -*-
"""MFCC_gru_0531

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QAByoMtR2ENhd1CyIfXyhPPnmq2cIfhs

#import
"""

import pandas as pd
import numpy as np
import librosa
import os
import tensorflow as tf
from sklearn.model_selection import train_test_split

from tensorflow import keras
# from tensorflow.keras.utils import to_categorical
from keras.utils import to_categorical

from sklearn.preprocessing import LabelEncoder
import random

from keras.utils import pad_sequences

# from tensorflow.keras.losses import CategoricalCrossentropy
from keras.losses import CategoricalCrossentropy
# from tensorflow.keras.optimizers import Adam,RMSprop
from keras.optimizers import Adam,RMSprop

# from tensorflow.keras.models import Sequential
from keras.models import Sequential
# from tensorflow.keras.layers import GRU, Dense, Dropout
from keras.layers import GRU, Dense, Dropout

# from google.colab import drive

# drive.mount('/content/drive', force_remount=True)

###GPU 러닝
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""#MFCC"""

#MFCC_python의 librosa 라이브러리 이용
import librosa
import librosa.display
import IPython.display as ipd
import os

# WavPath = '/content/drive/MyDrive/wav48'
WavPath='C:\\Users\\student\\Desktop\\Team2\\VCTK-Corpus\\VCTK-Corpus\\wav48'
print(os.listdir(WavPath))

def pad_sequences(sequences, max_length=None, padding_value=0):
        if max_length is None:
            max_length = max([len(seq) for seq in sequences])

        padded_sequences = np.full((len(sequences), max_length, sequences[0].shape[0]), padding_value, dtype=np.float32)
        for i, seq in enumerate(sequences):
            seq = seq.T  # Transpose dimensions
            padded_sequences[i, :len(seq), :] = seq[:max_length]

        return padded_sequences

#from error_Shape of sample (440,) of sequence at position 1 is different from expected shape (261,)>수정본
def pad_sequences(sequences, max_length=None, padding_value=0):
    if max_length is None:
        max_length = max([len(seq) for seq in sequences])


    padded_sequences = np.full((len(sequences), max_length, sequences[0].shape[0]), padding_value, dtype=np.float32)
    for i, seq in enumerate(sequences):
        seq = seq.T  # Transpose dimensions
        seq = seq[:max_length]  # Truncate sequence to max_length
        if seq.shape[0] < max_length:
            seq = np.pad(seq, [(0, max_length - seq.shape[0]), (0, 0)], mode='constant')
        padded_sequences[i, :len(seq), :] = seq

    return padded_sequences

"""#Dataset_load"""


def VCTKdataload(wavPath,num,sample_rate,resample_rate):

      
    Fils1 = os.listdir(wavPath)
    if num == 0:
      num = len(Fils1)

    wavedata = {}
    wavelen = []
    wavelabel_train = np.array([])
    wavelabel_test = np.array([])
    
    labels =  pd.DataFrame()
    print(Fils1)
    for i in Fils1[:num]:
      Fils2 = os.listdir(wavPath+'/'+i)
      if '.ipynb_checkpoints' in Fils2:
        Fils2.remove('.ipynb_checkpoints')
      print(Fils2)
      
      wavedata[i] = []
      id=[]
      fname=[]
      
      for j in Fils2:
        w,sr = librosa.load(wavPath+'/'+i+'/'+j, sr = sample_rate)
        rw = librosa.resample(w, orig_sr = sample_rate ,target_sr = resample_rate)
        D = np.abs(librosa.stft(rw, n_fft = 1200, win_length = 1200, hop_length = 240))
        mfcc = librosa.feature.mfcc(S = librosa.power_to_db(D), sr = sr, n_mfcc = 13) #MFCC 추가
        wavedata[i].append(mfcc)
        id.append(j.split('_')[0])
        fname.append(j)                                                   #wavdata 추출
   
      df =pd.DataFrame({'ID':id,'fname':fname})
      labels = pd.concat([labels, df], axis=0)
   
    labels = labels.loc[labels['ID'].isin(Fils1[:num])]
    labels = labels.reset_index(drop=True)
   
    y=labels['ID'].values

    encoder = LabelEncoder()
    y = encoder.fit_transform(y)

    y = np.array(y)                                    #레이블

    for i in Fils1[:num]:
      
      a = max([len(seq) for seq in wavedata[i]])
      wavelen.append(a)

    # max_length = max(wavelen)
    max_length=100
    print(max_length)

    for idx,i in enumerate(Fils1[:num]):
      b = pad_sequences(wavedata[i], max_length)
      print(b.shape)             #제로패딩
      
      x_train,x_test,y_train,y_test = train_test_split(b,y[np.where(y==int(idx))],test_size = 0.20, random_state=42)  

      x_train = tf.expand_dims(x_train, axis=-1)
      x_test = tf.expand_dims(x_test, axis=-1)
      print('train_tensor:',idx,x_train.shape)

      wavelabel_train = np.append(wavelabel_train,y_train)
      wavelabel_test = np.append(wavelabel_test,y_test)

      
      if idx == 0:
        wavetensor_train = x_train
        wavetensor_test = x_test
      
      else:
        wavetensor_train = tf.concat([wavetensor_train,x_train],0)
        wavetensor_test = tf.concat([wavetensor_test,x_test],0)
    
    train_labels_categorical = to_categorical(wavelabel_train)
    test_labels_categorical = to_categorical(wavelabel_test)
    
    
    
    print('traintensor shape:',wavetensor_train.shape)
    print('testtensor shape:',wavetensor_test.shape)
    print('trainlabel shape:',train_labels_categorical.shape)
    print('testlabel shape:',test_labels_categorical.shape)

    return wavetensor_train,wavetensor_test,train_labels_categorical,test_labels_categorical

wavPath = 'C:\\Users\\student\\Desktop\\Team2\\VCTK-Corpus\\VCTK-Corpus\\wav48'

# x_train,x_test,y_train,y_test = VCTKdataload(wavPath,20,48000,100)
x_train,x_test,y_train,y_test = VCTKdataload(wavPath,10,48000,48000)

"""#GRU"""


num=10
sample_rate=48000
resample_rate=24000
max_length=100
num_features=13

# x_train

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))

# Verify the new shape
print(x_train.shape)  # (batch_size, timesteps, features)

model = Sequential()


# Build your model architecture
model = Sequential()
model.add(GRU(units=20, activation='relu', return_sequences=True,input_shape=(max_length, num_features)))
# First GRU layer with ReLU activation
# model.add(GRU(20, activation='relu', return_sequences=True, input_shape=(100,13,1)))

# Intermediate GRU layers with ReLU activation
for _ in range(1, 4):
    model.add(GRU(20, activation='relu', return_sequences=True))
    # model.add(GRU(20, return_sequences=True))

# Final GRU layer with softmax activation
model.add(GRU(20, activation='relu', return_sequences=False))
# model.add(GRU(20, return_sequences=False))

# Output layer with softmax activation
model.add(Dense(10, activation='softmax')) #식별할 class의 수

# optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.01, epsilon=None, decay=0.9)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.003)

model.compile(loss='CategoricalCrossentropy', optimizer=optimizer, metrics=['accuracy'])


# Train the model
model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))

model.summary()

model.evaluate(x_test,y_test,verbose=1)
